{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Judge as a means to resolve disagreement\n",
    "\n",
    "- can we talk about honest and adversarial agents in the context of AI safety?\n",
    "- either we accept that agents may want to deceive us, then we can't assume even 1 honest agent\n",
    "- or we can decide to assume both of the agents are honest\n",
    "- still, even if both agents are acting in a good faith there will be cases of disagreement\n",
    "- how to combine results of different models is a known problem, usually solved by ensembling\n",
    "- could the game of debate be used as an ensembling method?\n",
    "- in this work I will explore the simplified version of the game with just the pre-commitmend phase, without the actual debate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from precommit_analysis.keras_mnist_example import prepare_data, create_mnist_cnn_model\n",
    "from precommit_analysis.generators import sparse_mnist_generator_nonzero\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "def get_accuracy(y_pred, y_true):\n",
    "    correct = (y_pred == y_true).sum()\n",
    "    print('correct: ', correct)\n",
    "    return correct / y_true.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test, input_shape = prepare_data(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_generator = sparse_mnist_generator_nonzero(\n",
    "    x_test,\n",
    "    y_test,\n",
    "    batch_size=x_test.shape[0],\n",
    "    sparsity=6,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x_sparse, data_y = next(val_data_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_categories = data_y.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Judge - 5k batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = create_mnist_cnn_model(num_classes, input_shape)\n",
    "judge.load_weights('models/model_sparse_mnist_generator_nonzero_5k.h5py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better judge - 30k batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = create_mnist_cnn_model(num_classes, input_shape)\n",
    "judge.load_weights('model_sparse_mnist_generator_nonzero_30k.h5py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The strategy\n",
    "\n",
    "- let's have a simple strategy for combining opinions of 2 agents with superior capabilities and a judge of limited capabilities\n",
    "- if the agents agree, take their classification as a result\n",
    "- if they disagree, take their opinions as a preselection (or pre-commitment to be consistent in terminology with the previous work) of candidate solutions\n",
    "- and let the judge decide which of the two is more likely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_disagreement(predictions_a, predictions_b, predictions_judge):\n",
    "    disagreement = predictions_a != predictions_b\n",
    "\n",
    "    resolution = predictions_judge[disagreement, predictions_a[disagreement]] > \\\n",
    "                 predictions_judge[disagreement, predictions_b[disagreement]]\n",
    "\n",
    "    # take b's predictions\n",
    "    result = predictions_b[disagreement]\n",
    "    \n",
    "    # unless a has a greater probability according to the judge\n",
    "    result[resolution] = predictions_a[disagreement][resolution]\n",
    "    return result, disagreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load two different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_agent_a = create_mnist_cnn_model(num_classes, input_shape)\n",
    "super_agent_a.load_weights('models/model_mnist_1epoch_adam1e-5.h5py')\n",
    "\n",
    "super_agent_b = create_mnist_cnn_model(num_classes, input_shape)\n",
    "super_agent_b.load_weights('models/model_mnist_1epoch_adam5e-5.h5py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_a = super_agent_a.predict(x_test).argmax(axis=1) # categorical\n",
    "predictions_b = super_agent_b.predict(x_test).argmax(axis=1) # categorical\n",
    "predictions_judge = judge.predict(data_x_sparse) # raw class probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find and resolve disagreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, disagreement = resolve_disagreement(predictions_a, predictions_b, predictions_judge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the accuracy of the agents alone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  8387\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8387"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(predictions_a, true_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  9368\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9368"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(predictions_b, true_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is their accuracy on the samples where they disagree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  94\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0722521137586472"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(predictions_a[disagreement], true_categories[disagreement])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  1075\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8262874711760184"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(predictions_b[disagreement], true_categories[disagreement])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the judge's accuracy on the samples where the agents disagree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_category_judge = predictions_judge.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  291\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.22367409684857803"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(predicted_category_judge[disagreement], true_categories[disagreement])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does it change if the agent's pre-commitments are combined with the judge's probabilities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  563\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4327440430438125"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(result, true_categories[disagreement])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_category_judge = predictions_judge.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1301"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disagreement.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents of the same power (just different seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_agent_a = create_mnist_cnn_model(num_classes, input_shape)\n",
    "super_agent_a.load_weights('models/model_mnist_1epoch_adam5e-5_2.h5py')\n",
    "\n",
    "super_agent_b = create_mnist_cnn_model(num_classes, input_shape)\n",
    "super_agent_b.load_weights('models/model_mnist_1epoch_adam5e-5.h5py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_a = super_agent_a.predict(x_test).argmax(axis=1)\n",
    "predictions_b = super_agent_b.predict(x_test).argmax(axis=1)\n",
    "predictions_judge = judge.predict(data_x_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find and resolve disagreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, disagreement = resolve_disagreement(predictions_a, predictions_b, predictions_judge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the accuracy of the agents alone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  9229\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9229"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(predictions_a, true_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  9368\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9368"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(predictions_b, true_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is their accuracy on the samples where they disagree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  86\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.23822714681440443"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(predictions_a[disagreement], true_categories[disagreement])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  225\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6232686980609419"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(predictions_b[disagreement], true_categories[disagreement])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the judge's accuracy on the samples where the agents disagree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_category_judge = predictions_judge.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  68\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1883656509695291"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(predicted_category_judge[disagreement], true_categories[disagreement])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does it change if the agent's pre-commitments are combined with the judge's probabilities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3961218836565097"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(result, true_categories[disagreement])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How big is the disagreement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "361"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the test set has 10k samples\n",
    "disagreement.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's the resulting accuracy of the agents plus the judge's resolution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds_combined = predictions_a.copy()\n",
    "all_preds_combined[disagreement] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  9286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9286"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(all_preds_combined, true_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary results\n",
    "\n",
    "- the pre-selection of two candidate classes by two superior, but fallible agents, seems to improve the accuracy of the limited judge\n",
    "- the judge's accuracy is significantly improved even when we examine only the samples, where the agents disagree (22.36% to 43.27%, resp. 18.84% to 39.61% in the second experiment)\n",
    "\n",
    "## What to do next\n",
    "- experiments should be repeated and evaluated on mean values, as the judge's results are stochastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
