{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Judge as a means to resolve disagreement\n",
    "\n",
    "- can we talk about honest and adversarial agents in the context of AI safety?\n",
    "- either we accept that agents may want to deceive us, then we can't assume even 1 honest agent\n",
    "- or we can decide to assume both of the agents are honest\n",
    "- still, even if both agents are acting in a good faith there will be cases of disagreement\n",
    "- how to combine results of different models is a known problem, usually solved by ensembling\n",
    "- could the game of debate be used as an ensembling method?\n",
    "- in this work I will explore the simplified version of the game with just the pre-commitmend phase, without the actual debate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hrosspet/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from utils import prepare_data, create_mnist_cnn_model, sparse_mnist_generator_nonzero\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "def get_accuracy(y_pred, y_true):\n",
    "    correct = (y_pred == y_true).sum()\n",
    "    print('correct: ', correct)\n",
    "    return correct / y_true.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test, input_shape = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_generator = sparse_mnist_generator_nonzero(\n",
    "    x_test,\n",
    "    y_test,\n",
    "    batch_size=x_test.shape[0],\n",
    "    sparsity=6,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x_sparse, data_y = next(val_data_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_categories = data_y.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Judge - 5k batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = create_mnist_cnn_model(num_classes, input_shape)\n",
    "judge.load_weights('model_sparse_mnist_generator_nonzero_5k.h5py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better judge - 30k batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = create_mnist_cnn_model(num_classes, input_shape)\n",
    "judge.load_weights('model_sparse_mnist_generator_nonzero_30k.h5py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The strategy\n",
    "\n",
    "- let's have a simple strategy for combining opinions of 2 agents with superior capabilities and a judge of limited capabilities\n",
    "- if the agents agree, take their classification as a result\n",
    "- if they disagree, take their opinions as a preselection (or pre-commitment to be consistent in terminology with the previous work) of candidate solutions\n",
    "- and let the judge decide which of the two is more likely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_disagreement(predictions_a, predictions_b, predictions_judge):\n",
    "    disagreement = predictions_a != predictions_b\n",
    "\n",
    "    resolution = predictions_judge[disagreement, predictions_a[disagreement]] > \\\n",
    "                 predictions_judge[disagreement, predictions_b[disagreement]]\n",
    "\n",
    "    # take b's predictions\n",
    "    result = predictions_b[disagreement]\n",
    "    \n",
    "    # unless a has a greater probability according to the judge\n",
    "    result[resolution] = predictions_a[disagreement][resolution]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load two different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_agent_a = create_mnist_cnn_model(num_classes, input_shape)\n",
    "super_agent_a.load_weights('model_mnist_1epoch_adam1e-5.h5py')\n",
    "\n",
    "super_agent_b = create_mnist_cnn_model(num_classes, input_shape)\n",
    "super_agent_b.load_weights('model_mnist_1epoch_adam5e-5.h5py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_a = super_agent_a.predict(x_test).argmax(axis=1) # categorical\n",
    "predictions_b = super_agent_b.predict(x_test).argmax(axis=1) # categorical\n",
    "predictions_judge = judge.predict(data_x_sparse) # raw class probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the accuracy of the agents alone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8387"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(predictions_a, true_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9368"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(predictions_b, true_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is their accuracy on the samples where they disagree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0722521137586472"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(predictions_a[disagreement], true_categories[disagreement])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8262874711760184"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(predictions_b[disagreement], true_categories[disagreement])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the judge's accuracy on the samples where the agents disagree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_category_judge = predictions_judge.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22982321291314373"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(predicted_category_judge[disagreement], true_categories[disagreement])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does it change if the agent's pre-commitments are combined with the judge's probabilities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44427363566487316"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = resolve_disagreement(predictions_a, predictions_b, predictions_judge)\n",
    "get_accuracy(result, true_categories[disagreement])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_category_judge = predictions_judge.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1301"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disagreement.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents of the same power (just different seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_agent_a = create_mnist_cnn_model(num_classes, input_shape)\n",
    "super_agent_a.load_weights('model_mnist_1epoch_adam5e-5_2.h5py')\n",
    "\n",
    "super_agent_b = create_mnist_cnn_model(num_classes, input_shape)\n",
    "super_agent_b.load_weights('model_mnist_1epoch_adam5e-5.h5py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x_sparse, data_y = next(val_data_generator)\n",
    "\n",
    "true_categories = data_y.argmax(axis=1)\n",
    "\n",
    "predictions_a = super_agent_a.predict(x_test).argmax(axis=1)\n",
    "predictions_b = super_agent_b.predict(x_test).argmax(axis=1)\n",
    "predictions_judge = judge.predict(data_x_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the accuracy of the agents alone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9229"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(predictions_a, true_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9368"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(predictions_b, true_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is their accuracy on the samples where they disagree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  86\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.23822714681440443"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(predictions_a[disagreement], true_categories[disagreement])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  225\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6232686980609419"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(predictions_b[disagreement], true_categories[disagreement])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the judge's accuracy on the samples where the agents disagree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_category_judge = predictions_judge.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21329639889196675"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(predicted_category_judge[disagreement], true_categories[disagreement])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does it change if the agent's pre-commitments are combined with the judge's probabilities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  137\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.37950138504155123"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = resolve_disagreement(predictions_a, predictions_b, predictions_judge)\n",
    "get_accuracy(result, true_categories[disagreement])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How big is the disagreement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "361"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the test set has 10k samples\n",
    "disagreement.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's the resulting accuracy of the agents plus the judge's resolution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds_combined = predictions_a.copy()\n",
    "all_preds_combined[disagreement] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  9280\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.928"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(all_preds_combined, true_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary results\n",
    "\n",
    "- the pre-selection of two candidate classes by two superior, but fallible agents, seems to improve the accuracy of the limited judge\n",
    "- the judge's accuracy is significantly improved even when we examine only the samples, where the agents disagree (22.98% to 44.43%, resp. 23.82% to 37.95% in the second experiment)\n",
    "\n",
    "## What to do next\n",
    "- experiments should be repeated and evaluated on mean values, as the judge's results are stochastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
