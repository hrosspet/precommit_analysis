{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from precommit_analysis.keras_mnist_example import prepare_data, create_mnist_cnn_model\n",
    "from precommit_analysis.generators import eval_generator, eval_precommit_generator, sparse_mnist_generator_nonzero, eval_precommit_adversarial_generator, eval_optimal_adversary_generator\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test, input_shape = prepare_data(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_generator = sparse_mnist_generator_nonzero(\n",
    "    x_test,\n",
    "    y_test,\n",
    "    batch_size=x_test.shape[0],\n",
    "    sparsity=6,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate judge alone\n",
    "\n",
    "- judge is a sparse MNIST classifier\n",
    "- 6 non-zero pixels are randomly sampled from an input image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Judge - 5k batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = create_mnist_cnn_model(num_classes, input_shape)\n",
    "judge.load_weights('models/model_sparse_mnist_generator_nonzero_5k.h5py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 52.99%\n",
      "variance: 9.043600E-06\n"
     ]
    }
   ],
   "source": [
    "# judge samples 6 pixels on random -> we need to see more runs and look at mean and variance\n",
    "accuracies = eval_generator(val_data_generator, judge, num_repetitions=10)\n",
    "print('accuracy: %.2f%%' % (100 * np.mean(accuracies)))\n",
    "print('variance: %E' % np.var(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better judge - 30k batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = create_mnist_cnn_model(num_classes, input_shape)\n",
    "judge.load_weights('models/model_sparse_mnist_generator_nonzero_30k.h5py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 55.51%\n",
      "variance: 8.398400E-06\n"
     ]
    }
   ],
   "source": [
    "accuracies = eval_generator(val_data_generator, judge, num_repetitions=10)\n",
    "print('accuracy: %.2f%%' % (100 * np.mean(accuracies)))\n",
    "print('variance: %E' % np.var(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random pre-commit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Judge - 5k batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 87.36%\n",
      "variance: 7.724500E-06\n"
     ]
    }
   ],
   "source": [
    "accuracies = eval_precommit_generator(val_data_generator, judge, num_classes, num_repetitions=10)\n",
    "print('accuracy: %.2f%%' % (100 * np.mean(accuracies)))\n",
    "print('variance: %E' % np.var(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better judge - 30k batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 88.31%\n",
      "variance: 8.738100E-06\n"
     ]
    }
   ],
   "source": [
    "accuracies = eval_precommit_generator(val_data_generator, judge, num_classes, num_repetitions=10)\n",
    "print('accuracy: %.2f%%' % (100 * np.mean(accuracies)))\n",
    "print('variance: %E' % np.var(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial precommit\n",
    "\n",
    "- evaluate the best adversary, which was found in train_adversary.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversary = create_mnist_cnn_model(num_classes, input_shape)\n",
    "adversary.load_weights('models/model_mnist_1epoch_adam1e-5.h5py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Judge - 5k batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 73.87%\n",
      "variance: 7.136900E-06\n"
     ]
    }
   ],
   "source": [
    "accuracies = eval_precommit_adversarial_generator(x_test, val_data_generator, judge, adversary, num_repetitions=10)\n",
    "print('accuracy: %.2f%%' % (100 * np.mean(accuracies)))\n",
    "print('variance: %E' % np.var(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better judge - 30k batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 75.41%\n",
      "variance: 1.368610E-05\n"
     ]
    }
   ],
   "source": [
    "accuracies = eval_precommit_adversarial_generator(x_test, val_data_generator, judge, adversary, num_repetitions=10)\n",
    "print('accuracy: %.2f%%' % (100 * np.mean(accuracies)))\n",
    "print('variance: %E' % np.var(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal adversary - perfect knowledge of judge\n",
    "\n",
    "- with perfect knowledge of the judge it's trivial to find an optimal adversarial pre-commit class\n",
    "- choose judge's predicted categories as long as they are not true\n",
    "- otherwise take the 2nd most probable class according to the judge and hope for a tie, which is a loose in our setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Judge - 5k batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 52.89%\n",
      "variance: 5.974100E-06\n"
     ]
    }
   ],
   "source": [
    "accuracies = eval_optimal_adversary_generator(val_data_generator, judge, num_repetitions=10)\n",
    "print('accuracy: %.2f%%' % (100 * np.mean(accuracies)))\n",
    "print('variance: %E' % np.var(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better judge - 30k batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 55.62%\n",
      "variance: 3.117450E-05\n"
     ]
    }
   ],
   "source": [
    "accuracies = eval_optimal_adversary_generator(val_data_generator, judge, num_repetitions=10)\n",
    "print('accuracy: %.2f%%' % (100 * np.mean(accuracies)))\n",
    "print('variance: %E' % np.var(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "- accuracy for different choices of pre-commit\n",
    "- highlighted are the highest accuracy for the judge with random 2nd class pre-commit\n",
    "- and the best adversarial pre-commit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| pre-commit type | judge 5k | judge 30k |\n",
    "|-----------------------|----------|----------|\n",
    "| **random** | **87.35%** | **88.31%** |\n",
    "| adversarial_top | 79.43% | 80.77% |\n",
    "| adversarial_30k | 77.72% | 80.60% |\n",
    "| adversarial_15k | 76.04% | 77.42% |\n",
    "| adversarial_10k | 75.31% | 76.82% |\n",
    "| adversarial_7.5k | 76.23% | 77.24% |\n",
    "| adversarial_5k | 78.31% | 80.12% |\n",
    "| adversarial_500 | 84.33% | 85.61% |\n",
    "|-----------------------|----------|----------|\n",
    "| adversarial_adam 1e-6 | 83.32% | 84.98% |\n",
    "| adversarial_adam 5e-5 | 75.23% | 76.28% \n",
    "| **adversarial_adam 1e-5** | **73.87%** | **75.41%** |\n",
    "| adversarial_adam 1e-4| 75.06% | 76.50% |\n",
    "|-----------------------|----------|----------|\n",
    "| perfect knowledge | 52.89% | 55.62% |\n",
    "|-----------------------|----------|----------|\n",
    "| none / baseline | 52.99% | 55.51% |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- much of the gain in judge's accuracy can be explained with the pre-commit only, without the actual debate between the 2 agents\n",
    "- adversarial precommit indeed managed to decrease the judge's accuracy compared to random precommit\n",
    "- the game of debate seems to me as a good tool for finding the candidate solutions (values for pre-commit) via agents of superior capabilities, and for mitigating the negative effect of the adversary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future work\n",
    "\n",
    "It's a question whether we can talk about honest and adversarial agents in the context of AI safety. Either we accept that agents may generally want to deceive us, then we can't assume even one honest agent. Or we can decide to assume both of the agents are honest. But even if both agents are acting in a good faith there will be cases of disagreement. This is a known problem usually solved by ensembling methods. Could the game of debate be used as an addition to the existing ensembling methods? If you want to see some preliminary exploration of this idea, take a look at the future_work.ipynb notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
